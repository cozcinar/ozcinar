+++
# Pub_type key
# 0 -> 'Forthcoming',
# 1 -> 'Preprint',
# 2 -> 'Journal',
# 3 -> 'Conference Proceedings',
# 4 -> 'Book chapter',
# 5 -> 'Thesis'

title = "Audio-visual perception of omnidirectional video for virtual reality applications"
date = "2020-07-10"
authors = ["Fang-Yi Chao", "Cagri Ozcinar", "C. Wang", "Emin Zerman", "L. Zhang", "W. Hamidouche", "O. Deforges", "Aljosa Smolic"]
publication_types = ["3"]
publication = "In *IEEE International Conference on Multimedia & Expo Workshops (ICMEW)2020*"
image_preview = ""
selected = true
projects = []
math = true
highlight = true
abstract= "Ambisonics, which constructs a sound distribution over the full viewing sphere, improves immersive experience in omnidirectional video (ODV) by enabling observers to perceive the sound directions. Thus, human attention could be guided by audio and visual stimuli simultaneously. Numerous datasets have been proposed to investigate human visual attention by collecting eye fixations of observers navigating ODV with head-mounted displays (HMD). However, there is no such dataset analyzing the impact of audio information. In this paper, we establish a new audio-visual attention dataset for ODV with mute, mono, and ambisonics. The user behavior including visual attention corresponding to sound source locations, viewing navigation congruence between observers and fixations distributions in these three audio modalities is studied based on video and audio content. From our statistical analysis, we preliminarily found that, compared to only perceiving visual cues perceiving visual cues with salient object sound (human voice, siren of ambulance) could draw more visual attention to the objects making sound and guide viewing behaviour when such objects are not in the current field of view. The more in-depth interactive effects between audio and visual cues in mute, mono and ambisonics still require further comprehensive study. The dataset and developed testbed in this initial work will be publicly available with the paper to foster future research on audio-visual attention for ODV."

# Links (optional).
url_pdf = ""
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = "https://github.com/cozcinar/360_Audio_Visual_ICMEW2020"

# Custom links (optional).
#   Uncomment line below to enable. For multiple links, use the form `[{...}, {...}, {...}]`.
#[[url_custom]]
#name = "View Proceeding Article"
#url = "https://arxiv.org/abs/1908.04297"

[header]
#image = "publications/alicke2015conceptions.png"
caption = ""
+++


